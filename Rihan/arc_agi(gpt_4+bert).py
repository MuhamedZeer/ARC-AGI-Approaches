# -*- coding: utf-8 -*-
"""ARC-AGI(gpt-4+bert).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvVXkzWLfEslOLmxsjFu_BZzfMDoIVLj

**In this notebook I have tried GPT-4 and BERT**

---

# GPT-4 for language transformation
"""

pip install openai

import os

file_path = "/Users/rehanalqatani/ARC/data/training/007bbfb7.json"

if os.path.exists(file_path):
    print("✅ File exists! You're good to go.")
else:
    print("❌ File not found. Please double-check the path.")

from google.colab import files
uploaded = files.upload()

from openai import OpenAI
import json
from google.colab import drive

# ✅ Step 0: Mount Google Drive
drive.mount('/content/drive')

# ✅ Step 1: Initialize GPT-4 client
client = OpenAI(api_key="")

# ✅ Step 2: Set path to the puzzle file (Google Drive location)
arc_file_path = "/content/drive/MyDrive/007bbfb7.json"  # ← make sure this is correct

# ✅ Step 3: Load the puzzle
with open(arc_file_path, "r") as f:
    data = json.load(f)

input_grid = data["train"][0]["input"]
output_grid = data["train"][0]["output"]

# ✅ Step 4: Grid to readable string
def grid_to_string(grid):
    return "\n".join(" ".join(str(cell) for cell in row) for row in grid)

# ✅ Step 5: Build GPT-4 prompt
prompt = f"""
This is an ARC reasoning task.

Input Grid:
{grid_to_string(input_grid)}

Output Grid:
{grid_to_string(output_grid)}

Describe in simple terms what transformation is applied from input to output.
Be logical and precise.
"""

# ✅ Step 6: Query GPT-4
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": prompt}
    ]
)

# ✅ Step 7: Show result
print("=== GPT-4 Transformation Description ===")
print(response.choices[0].message.content)

from openai import OpenAI
import json
from google.colab import drive

# ✅ Step 0: Mount Google Drive
drive.mount('/content/drive')

# ✅ Step 1: Initialize GPT-4 client
client = OpenAI(api_key="")

# ✅ Step 2: Set path to the puzzle file (Google Drive location)
arc_file_path = "/content/drive/MyDrive/ff805c23.json"  # ← make sure this is correct

# ✅ Step 3: Load the puzzle
with open(arc_file_path, "r") as f:
    data = json.load(f)

input_grid = data["train"][0]["input"]
output_grid = data["train"][0]["output"]

# ✅ Step 4: Grid to readable string
def grid_to_string(grid):
    return "\n".join(" ".join(str(cell) for cell in row) for row in grid)

# ✅ Step 5: Build GPT-4 prompt
prompt = f"""
This is an ARC reasoning task.

Input Grid:
{grid_to_string(input_grid)}

Output Grid:
{grid_to_string(output_grid)}

Describe in simple terms what transformation is applied from input to output.
Be logical and precise.
"""

# ✅ Step 6: Query GPT-4
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": prompt}
    ]
)

# ✅ Step 7: Show result
print("=== GPT-4 Transformation Description ===")
print(response.choices[0].message.content)

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os
DATASET_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data"
TRAIN_DIR = os.path.join(DATASET_DIR, "training")
EVAL_DIR = os.path.join(DATASET_DIR, "evaluation")

from openai import OpenAI
import json

client = OpenAI(api_key="")

def describe_transformation(input_grid, output_grid):
    def grid_to_string(grid):
        return "\n".join(" ".join(str(cell) for cell in row) for row in grid)

    prompt = f"""
This is an ARC reasoning task.

Input Grid:
{grid_to_string(input_grid)}

Output Grid:
{grid_to_string(output_grid)}

Describe the transformation in clear and concise terms.
Then conclude with:
Likely operation: <operation_name>

Examples of operation_name include: recolor, rotate, mirror, scale, shift, crop, count, etc.
Avoid unnecessary details. Be logical.
"""

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )

    return response.choices[0].message.content

import numpy as np

def identity(grid):
    return grid

def rotate90(grid):
    return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid):
    return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid):
    return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid):
    return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid):
    return np.flipud(np.array(grid)).tolist()

def recolor(grid, old_color, new_color):
    return [[new_color if cell == old_color else cell for cell in row] for row in grid]

def crop(grid, top, left, height, width):
    return [row[left:left+width] for row in grid[top:top+height]]

def paste(grid, obj, top, left):
    g = [row.copy() for row in grid]
    for i, row in enumerate(obj):
        for j, val in enumerate(row):
            g[top+i][left+j] = val
    return g

correct = 0
total = 0

for filepath in glob.glob(os.path.join(TRAIN_DIR, "*.json")):
    with open(filepath, "r") as f:
        data = json.load(f)

    input_grid = data["train"][0]["input"]
    output_grid = data["train"][0]["output"]

    gpt_response = describe_transformation(input_grid, output_grid)

    print(f"\nFile: {os.path.basename(filepath)}")
    print("GPT-4 Description:")
    print(gpt_response)

    match = re.search(r"Likely operation:\s*(\w+)", gpt_response)
    if not match:
        print("⚠️ Could not extract rule.")
        continue

    rule = match.group(1).lower()

    test_input = data["test"][0]["input"]
    test_expected = data["test"][0]["output"]

    try:
        if rule == "rotate":
            test_result = rotate90(test_input)
        elif rule == "identity":
            test_result = identity(test_input)
        elif rule == "recolor":
            # You need to define actual old/new color, here dummy:
            test_result = recolor(test_input, old_color=2, new_color=5)
        elif rule == "flip_horizontal":
            test_result = flip_horizontal(test_input)
        elif rule == "flip_vertical":
            test_result = flip_vertical(test_input)
        elif rule == "rotate180":
            test_result = rotate180(test_input)
        elif rule == "rotate270":
            test_result = rotate270(test_input)
        else:
            print(f"⚠️ Unsupported rule: {rule}")
            continue
    except Exception as e:
        print(f"❌ Error applying rule: {e}")
        continue

    if test_result == test_expected:
        correct += 1
    total += 1

# ✅ Final accuracy check
if total > 0:
    print(f"\n✅ Final Accuracy: {correct}/{total} ({100 * correct / total:.2f}%)")
else:
    print("\n⚠️ No tasks evaluated — possibly due to rule mismatch or missing DSL.")

import numpy as np

# === DSL Rule Functions ===

def identity(grid):
    return grid

def rotate90(grid):
    return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid):
    return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid):
    return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid):
    return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid):
    return np.flipud(np.array(grid)).tolist()

def recolor(grid, old_color, new_color):
    return [[new_color if cell == old_color else cell for cell in row] for row in grid]

def crop(grid, top, left, height, width):
    return [row[left:left+width] for row in grid[top:top+height]]

def paste(grid, obj, top, left):
    g = [row.copy() for row in grid]
    for i, row in enumerate(obj):
        for j, val in enumerate(row):
            g[top+i][left+j] = val
    return g

def translate(grid, dx, dy, fill_value=0):
    h, w = len(grid), len(grid[0])
    new_grid = [[fill_value for _ in range(w)] for _ in range(h)]
    for i in range(h):
        for j in range(w):
            ni, nj = i + dy, j + dx
            if 0 <= ni < h and 0 <= nj < w:
                new_grid[ni][nj] = grid[i][j]
    return new_grid

def remove_color(grid, color):
    return [[0 if cell == color else cell for cell in row] for row in grid]

def keep_only_color(grid, color):
    return [[cell if cell == color else 0 for cell in row] for row in grid]

def replace_color(grid, mapping):
    return [[mapping.get(cell, cell) for cell in row] for row in grid]

def color_border(grid, color):
    h, w = len(grid), len(grid[0])
    new_grid = [row.copy() for row in grid]
    for i in range(h):
        for j in range(w):
            if i == 0 or i == h-1 or j == 0 or j == w-1:
                new_grid[i][j] = color
    return new_grid

import os
import json

# ✅ DSL operation lookup
dsl_lookup = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
    "recolor": lambda grid: recolor(grid, 2, 5),  # change as needed
    "crop": lambda grid: crop(grid, 0, 0, 3, 3),  # example
    "paste": lambda grid: paste(grid, [[1]], 1, 1),
    "translate": lambda grid: translate(grid, 1, 0),
    "remove_color": lambda grid: remove_color(grid, 2),
    "keep_only_color": lambda grid: keep_only_color(grid, 2),
    "replace_color": lambda grid: replace_color(grid, {2: 5}),
    "color_border": lambda grid: color_border(grid, 3)
}

# ✅ Helper to extract operation from GPT-4 response
def extract_operation(description):
    for key in dsl_lookup.keys():
        if key in description.lower():
            return key
    return None

# ✅ GPT prompt builder
def build_prompt(in_grid, out_grid):
    return f"""
This is an ARC reasoning task.

Input Grid:
{grid_to_string(in_grid)}

Output Grid:
{grid_to_string(out_grid)}

Describe the transformation applied from input to output in simple and logical terms.
Finish with a clear statement:
Likely operation: <operation_name>
"""

# ✅ Evaluation loop
correct = 0
total = 0

# Change this to your real path
training_folder = "/content/drive/MyDrive/ARC-AGI-2-main/data/training"
test_folder = "/content/drive/MyDrive/ARC-AGI-2-main/data/evaluation"

for filename in os.listdir(training_folder):
    if not filename.endswith(".json"):
        continue

    train_path = os.path.join(training_folder, filename)
    test_path = os.path.join(test_folder, filename)

    if not os.path.exists(test_path):
        continue

    with open(train_path, "r") as f:
        data = json.load(f)

    train_pair = data["train"][0]
    in_grid = train_pair["input"]
    out_grid = train_pair["output"]

    prompt = build_prompt(in_grid, out_grid)

    # GPT-4 step
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    desc = response.choices[0].message.content
    op = extract_operation(desc)

    if op not in dsl_lookup:
        print(f"[{filename}] ❌ Unknown operation: {op}")
        continue

    # Apply to test input
    test_input = data["test"][0]["input"]
    expected_output = data["test"][0]["output"]

    try:
        predicted_output = dsl_lookup[op](test_input)
    except Exception as e:
        print(f"[{filename}] ⚠️ Error applying rule: {e}")
        continue

    if predicted_output == expected_output:
        correct += 1
    else:
        print(f"[{filename}] ❌ Incorrect prediction.")

    total += 1

# ✅ Final Accuracy
accuracy = (correct / total * 100) if total > 0 else 0
print(f"\n✅ Evaluation Accuracy: {accuracy:.2f}% ({correct}/{total})")

import os, json, numpy as np

# point this at your training split
training_folder = "/content/drive/MyDrive/ARC-AGI-2-main/data/training"

files = sorted(f for f in os.listdir(training_folder) if f.endswith(".json"))
print("Found", len(files), "training tasks.")

total = 0
correct = 0

for fn in files:
    data = json.load(open(os.path.join(training_folder, fn)))
    # now this will succeed
    in_grid  = data["train"][0]["input"]
    out_grid = data["train"][0]["output"]

    # …build prompt & call GPT-4 as before…
    # Make sure to parse “Likely operation:” correctly

    # lookup and apply your DSL_RULES[op]
    # compare pred vs out_grid, increment total/correct…

import numpy as np

# === DSL Rule Functions ===

def identity(grid):
    return grid

def rotate90(grid):
    return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid):
    return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid):
    return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid):
    return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid):
    return np.flipud(np.array(grid)).tolist()

def recolor(grid, old_color, new_color):
    return [[new_color if cell == old_color else cell for cell in row] for row in grid]

def crop(grid, top, left, height, width):
    return [row[left:left+width] for row in grid[top:top+height]]

def paste(grid, obj, top, left):
    g = [row.copy() for row in grid]
    for i, row in enumerate(obj):
        for j, val in enumerate(row):
            g[top+i][left+j] = val
    return g

def translate(grid, dx, dy, fill_value=0):
    h, w = len(grid), len(grid[0])
    new_grid = [[fill_value for _ in range(w)] for _ in range(h)]
    for i in range(h):
        for j in range(w):
            ni, nj = i + dy, j + dx
            if 0 <= ni < h and 0 <= nj < w:
                new_grid[ni][nj] = grid[i][j]
    return new_grid

def remove_color(grid, color):
    return [[0 if cell == color else cell for cell in row] for row in grid]

def keep_only_color(grid, color):
    return [[cell if cell == color else 0 for cell in row] for row in grid]

def replace_color(grid, mapping):
    return [[mapping.get(cell, cell) for cell in row] for row in grid]

def color_border(grid, color):
    h, w = len(grid), len(grid[0])
    new_grid = [row.copy() for row in grid]
    for i in range(h):
        for j in range(w):
            if i == 0 or i == h-1 or j == 0 or j == w-1:
                new_grid[i][j] = color
    return new_grid

from openai import OpenAI
import os
import json
import numpy as np

# === DSL Rule Functions ===

def identity(grid): return grid

def rotate90(grid): return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid): return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid): return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid): return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid): return np.flipud(np.array(grid)).tolist()

def recolor(grid, old_color, new_color):
    return [[new_color if cell == old_color else cell for cell in row] for row in grid]

def crop(grid, top, left, height, width):
    return [row[left:left+width] for row in grid[top:top+height]]

def paste(grid, obj, top, left):
    g = [row.copy() for row in grid]
    for i, row in enumerate(obj):
        for j, val in enumerate(row):
            g[top+i][left+j] = val
    return g

def translate(grid, dx, dy, fill_value=0):
    h, w = len(grid), len(grid[0])
    new = [[fill_value]*w for _ in range(h)]
    for i in range(h):
        for j in range(w):
            ni, nj = i+dy, j+dx
            if 0 <= ni < h and 0 <= nj < w:
                new[ni][nj] = grid[i][j]
    return new

def remove_color(grid, color): return [[0 if cell==color else cell for cell in row] for row in grid]

def keep_only_color(grid, color): return [[cell if cell==color else 0 for cell in row] for row in grid]

def replace_color(grid, mapping): return [[mapping.get(cell, cell) for cell in row] for row in grid]

def color_border(grid, color):
    h, w = len(grid), len(grid[0])
    new = [row.copy() for row in grid]
    for i in range(h):
        for j in range(w):
            if i in (0,h-1) or j in (0,w-1): new[i][j] = color
    return new

# Map DSL names to functions
dsl_rules = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
    # add wrappers for parametric functions as needed
}

# ------------------ CONFIGURATION ------------------
# Ensure your API key is set. Optionally, set it directly here (not recommended for shared code):
API_KEY = os.getenv("OPENAI_API_KEY")
# Uncomment and fill in below if you prefer hardcoding (keep secret!):
API_KEY = ""
if not API_KEY:
    raise ValueError("OPENAI_API_KEY is not set. Please set it in your environment or in the API_KEY variable.")

# Initialize GPT-4 client
client = OpenAI(api_key=API_KEY)

BASE_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data"
TRAIN_DIR = os.path.join(BASE_DIR, "training")
EVAL_DIR  = os.path.join(BASE_DIR, "evaluation")

# ------------------ GPT-4 PROMPT ------------------
def infer_rule(examples):
    msgs = [
        {"role": "system", "content": "Analyze each input/output grid pair and state, in one concise sentence, the transformation rule applied."}
    ]
    for i, (inp, out) in enumerate(examples, 1):
        msgs.append({
            "role": "user",
            "content": f"Pair {i}:\nInput:{inp}\nOutput:{out}\nDescribe the transformation."
        })
    resp = client.chat.completions.create(
        model="gpt-4",
        messages=msgs,
        temperature=0
    )
    return resp.choices[0].message.content.strip().lower()

# ------------------ TRAINING PHASE ------------------
discovered = {}
for fname in sorted(os.listdir(TRAIN_DIR)):
    if not fname.endswith('.json'): continue
    task_id = fname[:-5]
    path = os.path.join(TRAIN_DIR, fname)
    data = json.load(open(path))
    examples = [(ex['input'], ex['output']) for ex in data.get('train', [])]
    if not examples:
        print(f"{task_id}: no training pairs")
        continue
    desc = infer_rule(examples)
    print(f"{task_id}: GPT-4 -> {desc}")
    for name, func in dsl_rules.items():
        if name in desc:
            discovered[task_id] = func
            print(f"  matched DSL: {name}")
            break
    else:
        print(f"  no DSL match for: {desc}")

# ------------------ EVALUATION PHASE ------------------
correct = total = 0
for fname in sorted(os.listdir(EVAL_DIR)):
    if not fname.endswith('.json'): continue
    task_id = fname[:-5]
    data = json.load(open(os.path.join(EVAL_DIR, fname)))
    test = data.get('test', [])
    if not test:
        print(f"{task_id}: no test case")
        continue
    inp = test[0]['input']; actual = test[0]['output']
    func = discovered.get(task_id)
    if not func:
        print(f"{task_id}: skip (no rule)")
        continue
    pred = func(inp)
    total += 1
    if pred == actual:
        correct += 1
        print(f"{task_id}: correct")
    else:
        print(f"{task_id}: incorrect")

print(f"Accuracy: {correct}/{total} = {100*correct/total:.2f}%")

"""# Loading dataset+Defining rules+mapping"""

import json
d = json.load(open("/content/drive/MyDrive/ARC-AGI-2-main/data/training/c3e719e8.json"))
print(d.keys())

import os
TRAIN_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data/training"
EVAL_DIR  = "/content/drive/MyDrive/ARC-AGI-2-main/data/evaluation"

train_files = sorted(f for f in os.listdir(TRAIN_DIR) if f.endswith('.json'))
eval_files  = sorted(f for f in os.listdir(EVAL_DIR)  if f.endswith('.json'))

print("Training JSONs:", len(train_files), train_files[:10])
print("Eval     JSONs:", len(eval_files),  eval_files[:10])

import json
sample = train_files[0]
path   = os.path.join(TRAIN_DIR, sample)

with open(path) as f:
    data = json.load(f)

print("Type:", type(data))
print("Top‐level keys:", list(data.keys()))
print("First example entry (if list):", data.get('train')[:1])

sample = eval_files[0]
path   = os.path.join(EVAL_DIR, sample)

with open(path) as f:
    data = json.load(f)

print("Eval file keys:", list(data.keys()))
print("First test example:", data.get('test') or data.get('evaluation') or data[:1])

import os, json, numpy as np

# Copy in your DSL functions:
def identity(grid): return grid
def rotate90(grid): return np.rot90(np.array(grid), k=-1).tolist()
def rotate180(grid): return np.rot90(np.array(grid), k=2).tolist()
def rotate270(grid): return np.rot90(np.array(grid), k=1).tolist()
def flip_horizontal(grid): return np.fliplr(np.array(grid)).tolist()
def flip_vertical(grid): return np.flipud(np.array(grid)).tolist()

dsl_rules = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
}

TRAIN_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data/training"
# helper to find JSONs
train_files = {
    os.path.splitext(f)[0]: os.path.join(TRAIN_DIR, f)
    for f in os.listdir(TRAIN_DIR) if f.endswith('.json')
}

# same load_examples you have:
def load_examples(path, split_key='train'):
    d = json.load(open(path))
    ex_list = d.get(split_key, []) if isinstance(d, dict) else []
    return [(e['input'], e['output']) for e in ex_list if 'input' in e]

# build mapping
discovered = {}
for tid, path in train_files.items():
    exs = load_examples(path, 'train')
    for name, fn in dsl_rules.items():
        if all(fn(i)==o for i,o in exs):
            discovered[tid] = name
            break
    else:
        discovered[tid] = 'identity'

print("→ discovered_rules count:", len(discovered))
print("→ sample keys:", list(discovered)[:10])

import os

EVAL_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data/evaluation"
eval_files = [os.path.splitext(f)[0]
              for f in os.listdir(EVAL_DIR) if f.endswith('.json')]

print("→ eval_files count:", len(eval_files))
print("→ sample eval IDs:", eval_files[:10])

missing = [tid for tid in eval_files if tid not in discovered]
print(f"→ {len(missing)} eval IDs not in discovered_rules")
print("→ missing examples:", missing[:10])

import os, json, numpy as np

# re-define just your DSL fns:
def identity(g): return g
def rotate90(g): return np.rot90(np.array(g), k=-1).tolist()
def rotate180(g): return np.rot90(np.array(g), k=2).tolist()
def rotate270(g): return np.rot90(np.array(g), k=1).tolist()
def flip_horizontal(g): return np.fliplr(np.array(g)).tolist()
def flip_vertical(g): return np.flipud(np.array(g)).tolist()
dsl = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
}

TRAIN_DIR = "/content/drive/MyDrive/ARC-AGI-2-main/data/training"
EVAL_DIR  = "/content/drive/MyDrive/ARC-AGI-2-main/data/evaluation"

# find all JSONs
train_files = {os.path.splitext(f)[0]: os.path.join(TRAIN_DIR, f)
               for f in os.listdir(TRAIN_DIR) if f.endswith('.json')}
eval_files  = {os.path.splitext(f)[0]: os.path.join(EVAL_DIR,  f)
               for f in os.listdir(EVAL_DIR)  if f.endswith('.json')}

# loader for one split
def load_examples(path, split):
    d = json.load(open(path))
    exs = d.get(split, [])
    return [(e['input'], e['output']) for e in exs if 'input' in e]

# map over TRAIN then EVAL
discovered = {}
for split_map, key in [(train_files,'train'), (eval_files,'test')]:
    for tid, path in split_map.items():
        exs = load_examples(path, key)
        for name, fn in dsl.items():
            if all(fn(i)==o for i,o in exs):
                discovered[tid] = name
                break
        else:
            discovered[tid] = 'identity'

print(f"Mapped tasks total: {len(discovered)}\n"
      f"  should equal {len(train_files)+len(eval_files)}")

samples_test = []
for tid, path in eval_files.items():
    rule = discovered[tid]
    for inp, out in load_examples(path, 'test'):
        samples_test.append((inp, out, rule))

print("Number of test examples:", len(samples_test))
print("Example label set:", set(r for (_,_,r) in samples_test))

"""#distilber(BERT)

"""

import os
import json
import numpy as np
from datasets import Dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)

# === DSL Function Definitions ===
def identity(grid): return grid

def rotate90(grid): return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid): return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid): return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid): return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid): return np.flipud(np.array(grid)).tolist()

# Map non-parametric rule names to functions
dsl_rules = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
}

# === Configuration ===
MODEL_NAME = os.getenv("LLM_MODEL", "distilbert-base-uncased")
# Corrected folder structure: data/training and data/evaluation under arcagi2main
BASE_DIR   = "/content/drive/MyDrive/ARC-AGI-2-main"
TRAIN_DIR  = os.path.join(BASE_DIR, "data", "training")
EVAL_DIR   = os.path.join(BASE_DIR, "data", "evaluation")
OUTPUT_DIR = "./arc_agi_finetuned"

# === Helper: find all JSON files under a directory ===
def find_json_files(root_dir):
    files = {}
    for rd, _, fs in os.walk(root_dir):
        for f in fs:
            if f.endswith('.json'):
                tid = os.path.splitext(f)[0]
                files[tid] = os.path.join(rd, f)
    return files

train_files = find_json_files(TRAIN_DIR)
eval_files  = find_json_files(EVAL_DIR)

if not train_files:
    raise RuntimeError(f"No JSON files found under {TRAIN_DIR}")
if not eval_files:
    raise RuntimeError(f"No JSON files found under {EVAL_DIR}")

print(f"Found {len(train_files)} training tasks and {len(eval_files)} evaluation tasks.")

# === Utility to load examples from a task JSON ===
def load_examples(json_path, split_key):
    """Load (input, output) pairs for a given split key, handling various JSON structures."""
    data = json.load(open(json_path))
    # Attempt to extract list by split_key in various forms
    if isinstance(data, dict):
        ex_list = data.get(split_key) or data.get(split_key.upper()) or data.get('examples') or data.get('Examples') or []
        if not isinstance(ex_list, list):
            ex_list = []
    elif isinstance(data, list):
        ex_list = data
    else:
        ex_list = []
    # Build list of (input, output) tuples
    pairs = []
    for ex in ex_list:
        if isinstance(ex, dict) and 'input' in ex and 'output' in ex:
            pairs.append((ex['input'], ex['output']))
    return pairs

# === Auto-map DSL rules by matching both train and evaluation examples ===
# We include both 'train' and 'test' splits so every eval task gets a rule mapping
# Build mapping for training tasks
for split_map, split_key in [(train_files, 'train'), (eval_files, 'test')]:
    for tid, path in split_map.items():
        examples = load_examples(path, split_key)
        for name, fn in dsl_rules.items():
            if examples and all(fn(inp) == out for inp, out in examples):
                discovered_rules[tid] = name
                break
        else:
            # fallback to identity if no rule matches or no examples
            discovered_rules[tid] = 'identity'

print("Discovered DSL mapping (total tasks):", len(discovered_rules))
# show first few mappings
for i, (tid, rule) in enumerate(discovered_rules.items()):
    if i >= 10: break
    print(f"  {tid}: {rule}")

# === Prepare samples for fine-tuning ===
samples = {'train': [], 'test': []}
for split, file_map in [('train', train_files), ('test', eval_files)]:
    for tid, path in file_map.items():
        rule = discovered_rules.get(tid)
        if rule is None:
            continue
        for inp, out in load_examples(path, split):
            text = (
                f"Input:\n" + "\n".join(" ".join(str(c) for c in row) for row in inp) + "\n"
                f"Output:\n" + "\n".join(" ".join(str(c) for c in row) for row in out)
            )
            samples[split].append({"text": text, "label": list(dsl_rules.keys()).index(rule)})

if not samples['train'] or not samples['test']:
    raise RuntimeError("Training or test samples missing. Check your split keys.")

train_ds = Dataset.from_list(samples['train'])
test_ds  = Dataset.from_list(samples['test'])

def preprocess(batch):
    return tokenizer(batch['text'], truncation=True)

# === Load model & tokenizer ===
print(f"Loading model '{MODEL_NAME}'...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model     = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(dsl_rules),
    id2label={i:r for i,r in enumerate(dsl_rules)},
    label2id={r:i for i,r in enumerate(dsl_rules)},
)

train_tok = train_ds.map(preprocess, batched=True)
test_tok  = test_ds.map(preprocess, batched=True)

data_collator = DataCollatorWithPadding(tokenizer)
metric = load_metric('accuracy')
def compute_metrics(eval_pred):
    preds, labels = eval_pred
    preds = np.argmax(preds, axis=-1)
    return metric.compute(predictions=preds, references=labels)

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    learning_rate=5e-5,
    logging_steps=50,
    save_steps=500,
)



trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tok,
    eval_dataset=test_tok,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# === Train & Evaluate ===
trainer.train()
eval_res = trainer.evaluate()
print("Evaluation:", eval_res)

# count solved tasks
preds = trainer.predict(test_tok)
pred_labels = np.argmax(preds.predictions, axis=-1)
total = len(pred_labels)
correct = (pred_labels == preds.label_ids).sum()
print(f"Solved {correct}/{total} = {100*correct/total:.2f}% tasks.")

import os
import json
import numpy as np
from datasets import Dataset, load_metric
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)

# === DSL Function Definitions ===
def identity(grid): return grid

def rotate90(grid): return np.rot90(np.array(grid), k=-1).tolist()

def rotate180(grid): return np.rot90(np.array(grid), k=2).tolist()

def rotate270(grid): return np.rot90(np.array(grid), k=1).tolist()

def flip_horizontal(grid): return np.fliplr(np.array(grid)).tolist()

def flip_vertical(grid): return np.flipud(np.array(grid)).tolist()

# Map non-parametric rule names to functions
dsl_rules = {
    "identity": identity,
    "rotate90": rotate90,
    "rotate180": rotate180,
    "rotate270": rotate270,
    "flip_horizontal": flip_horizontal,
    "flip_vertical": flip_vertical,
}

# === Configuration ===
# Use a fully public classification model to avoid auth errors
MODEL_NAME = os.getenv("LLM_MODEL", "distilbert-base-uncased")  # a small HF model that doesn’t require an HF token

